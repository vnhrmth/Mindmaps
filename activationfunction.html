
<!doctype html>
<html>
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta http-equiv="X-UA-Compatible" content="ie=edge" />
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
html {
  font-family: ui-sans-serif, system-ui, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.markmap-dark {
  background: #27272a;
  color: white;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.12/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/index.js"></script><script>(()=>{setTimeout(()=>{const{markmap:S,mm:Q}=window,$=new S.Toolbar;$.attach(Q);const I=$.render();I.setAttribute("style","position:absolute;bottom:20px;right:20px"),document.body.append(I)})})()</script><script>((l,U,M,R)=>{const N=l();window.mm=N.Markmap.create("svg#mindmap",(U||N.deriveOptions)(R),M),window.matchMedia("(prefers-color-scheme: dark)").matches&&document.documentElement.classList.add("markmap-dark")})(()=>window.markmap,null,{"content":"&#x2699;&#xfe0f; Activation Functions","children":[{"content":"1. Purpose","children":[{"content":"Introduce non-linearity into neural networks","children":[],"payload":{"tag":"li","lines":"3,4"}},{"content":"Enable learning of complex patterns","children":[],"payload":{"tag":"li","lines":"4,5"}},{"content":"Control the output of neurons","children":[],"payload":{"tag":"li","lines":"5,7"}}],"payload":{"tag":"h2","lines":"2,3"}},{"content":"2. Categories","children":[{"content":"2.1 Linear Activation","children":[{"content":"<strong>Linear Function</strong>","children":[{"content":"Formula: f(x) = x","children":[],"payload":{"tag":"li","lines":"10,11"}},{"content":"Rarely used in deep networks","children":[],"payload":{"tag":"li","lines":"11,12"}},{"content":"No non-linearity &#x2192; limited learning capacity","children":[],"payload":{"tag":"li","lines":"12,14"}}],"payload":{"tag":"li","lines":"9,14"}}],"payload":{"tag":"h3","lines":"8,9"}},{"content":"2.2 Non-Linear Activation","children":[{"content":"2.2.1 Sigmoid Family","children":[{"content":"\n<p data-lines=\"17,18\"><strong>Sigmoid</strong></p>","children":[{"content":"Formula: f(x) = 1 / (1 + e^(-x))","children":[],"payload":{"tag":"li","lines":"18,19"}},{"content":"Output range: (0, 1)","children":[],"payload":{"tag":"li","lines":"19,20"}},{"content":"Pros: Smooth gradient, probabilistic interpretation","children":[],"payload":{"tag":"li","lines":"20,21"}},{"content":"Cons: Vanishing gradient, not zero-centered","children":[],"payload":{"tag":"li","lines":"21,23"}}],"payload":{"tag":"li","lines":"17,23"}},{"content":"\n<p data-lines=\"23,24\"><strong>Tanh (Hyperbolic Tangent)</strong></p>","children":[{"content":"Formula: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))","children":[],"payload":{"tag":"li","lines":"24,25"}},{"content":"Output range: (-1, 1)","children":[],"payload":{"tag":"li","lines":"25,26"}},{"content":"Pros: Zero-centered","children":[],"payload":{"tag":"li","lines":"26,27"}},{"content":"Cons: Still suffers from vanishing gradient","children":[],"payload":{"tag":"li","lines":"27,29"}}],"payload":{"tag":"li","lines":"23,29"}}],"payload":{"tag":"h4","lines":"16,17"}},{"content":"2.2.2 ReLU Family","children":[{"content":"\n<p data-lines=\"30,31\"><strong>ReLU (Rectified Linear Unit)</strong></p>","children":[{"content":"Formula: f(x) = max(0, x)","children":[],"payload":{"tag":"li","lines":"31,32"}},{"content":"Pros: Simple, efficient, reduces vanishing gradient","children":[],"payload":{"tag":"li","lines":"32,33"}},{"content":"Cons: Dying ReLU problem (neurons stuck at 0)","children":[],"payload":{"tag":"li","lines":"33,35"}}],"payload":{"tag":"li","lines":"30,35"}},{"content":"\n<p data-lines=\"35,36\"><strong>Leaky ReLU</strong></p>","children":[{"content":"Formula: f(x) = x if x &gt; 0 else &#x3b1;x (&#x3b1; small)","children":[],"payload":{"tag":"li","lines":"36,37"}},{"content":"Pros: Fixes dying ReLU","children":[],"payload":{"tag":"li","lines":"37,38"}},{"content":"Cons: &#x3b1; needs tuning","children":[],"payload":{"tag":"li","lines":"38,40"}}],"payload":{"tag":"li","lines":"35,40"}},{"content":"\n<p data-lines=\"40,41\"><strong>Parametric ReLU (PReLU)</strong></p>","children":[{"content":"Similar to Leaky ReLU but &#x3b1; is learned during training","children":[],"payload":{"tag":"li","lines":"41,43"}}],"payload":{"tag":"li","lines":"40,43"}},{"content":"\n<p data-lines=\"43,44\"><strong>ELU (Exponential Linear Unit)</strong></p>","children":[{"content":"Smooths the negative part","children":[],"payload":{"tag":"li","lines":"44,45"}},{"content":"Helps with faster convergence","children":[],"payload":{"tag":"li","lines":"45,47"}}],"payload":{"tag":"li","lines":"43,47"}}],"payload":{"tag":"h4","lines":"29,30"}},{"content":"2.2.3 Advanced / Specialized","children":[{"content":"\n<p data-lines=\"48,49\"><strong>Swish</strong></p>","children":[{"content":"Formula: f(x) = x * sigmoid(x)","children":[],"payload":{"tag":"li","lines":"49,50"}},{"content":"Developed by Google","children":[],"payload":{"tag":"li","lines":"50,51"}},{"content":"Smooth and non-monotonic","children":[],"payload":{"tag":"li","lines":"51,53"}}],"payload":{"tag":"li","lines":"48,53"}},{"content":"\n<p data-lines=\"53,54\"><strong>Mish</strong></p>","children":[{"content":"Formula: f(x) = x * tanh(softplus(x))","children":[],"payload":{"tag":"li","lines":"54,55"}},{"content":"Smooth, self-regularizing","children":[],"payload":{"tag":"li","lines":"55,57"}}],"payload":{"tag":"li","lines":"53,57"}},{"content":"\n<p data-lines=\"57,58\"><strong>Softmax</strong></p>","children":[{"content":"Used in output layer for classification","children":[],"payload":{"tag":"li","lines":"58,59"}},{"content":"Converts logits into probabilities","children":[],"payload":{"tag":"li","lines":"59,60"}},{"content":"Formula: f(x&#x1d62;) = e^(x&#x1d62;) / &#x3a3; e^(x&#x2c7c;)","children":[],"payload":{"tag":"li","lines":"60,62"}}],"payload":{"tag":"li","lines":"57,62"}}],"payload":{"tag":"h4","lines":"47,48"}}],"payload":{"tag":"h3","lines":"14,15"}}],"payload":{"tag":"h2","lines":"7,8"}},{"content":"3. Selection Criteria","children":[{"content":"<strong>Task Type</strong>","children":[{"content":"Classification &#x2192; Sigmoid, Softmax","children":[],"payload":{"tag":"li","lines":"64,65"}},{"content":"Regression &#x2192; Linear, ReLU","children":[],"payload":{"tag":"li","lines":"65,66"}}],"payload":{"tag":"li","lines":"63,66"}},{"content":"<strong>Layer Position</strong>","children":[{"content":"Hidden layers &#x2192; ReLU, Tanh","children":[],"payload":{"tag":"li","lines":"67,68"}},{"content":"Output layer &#x2192; Depends on task","children":[],"payload":{"tag":"li","lines":"68,69"}}],"payload":{"tag":"li","lines":"66,69"}},{"content":"<strong>Gradient Behavior</strong>","children":[{"content":"Avoid vanishing/exploding gradients","children":[],"payload":{"tag":"li","lines":"70,71"}}],"payload":{"tag":"li","lines":"69,71"}},{"content":"<strong>Computational Efficiency</strong>","children":[{"content":"Simpler functions preferred for speed","children":[],"payload":{"tag":"li","lines":"72,74"}}],"payload":{"tag":"li","lines":"71,74"}}],"payload":{"tag":"h2","lines":"62,63"}},{"content":"4. Challenges","children":[{"content":"<strong>Vanishing Gradient</strong>","children":[{"content":"Gradients become too small to update weights","children":[],"payload":{"tag":"li","lines":"76,77"}}],"payload":{"tag":"li","lines":"75,77"}},{"content":"<strong>Exploding Gradient</strong>","children":[{"content":"Gradients become too large, destabilizing training","children":[],"payload":{"tag":"li","lines":"78,79"}}],"payload":{"tag":"li","lines":"77,79"}},{"content":"<strong>Dead Neurons</strong>","children":[{"content":"Neurons stop activating (common in ReLU)","children":[],"payload":{"tag":"li","lines":"80,82"}}],"payload":{"tag":"li","lines":"79,82"}}],"payload":{"tag":"h2","lines":"74,75"}},{"content":"5. Applications","children":[{"content":"<strong>Image Processing</strong>","children":[{"content":"ReLU, Leaky ReLU","children":[],"payload":{"tag":"li","lines":"84,85"}}],"payload":{"tag":"li","lines":"83,85"}},{"content":"<strong>Natural Language Processing</strong>","children":[{"content":"Swish, GELU (used in Transformers)","children":[],"payload":{"tag":"li","lines":"86,87"}}],"payload":{"tag":"li","lines":"85,87"}},{"content":"<strong>Binary Classification</strong>","children":[{"content":"Sigmoid","children":[],"payload":{"tag":"li","lines":"88,89"}}],"payload":{"tag":"li","lines":"87,89"}},{"content":"<strong>Multi-Class Classification</strong>","children":[{"content":"Softmax","children":[],"payload":{"tag":"li","lines":"90,91"}}],"payload":{"tag":"li","lines":"89,91"}}],"payload":{"tag":"h2","lines":"82,83"}}],"payload":{"tag":"h1","lines":"0,1"}},null)</script>
</body>
</html>
