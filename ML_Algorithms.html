
<!doctype html>
<html>
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta http-equiv="X-UA-Compatible" content="ie=edge" />
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
html {
  font-family: ui-sans-serif, system-ui, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.markmap-dark {
  background: #27272a;
  color: white;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.12/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/index.js"></script><script>(()=>{setTimeout(()=>{const{markmap:S,mm:Q}=window,$=new S.Toolbar;$.attach(Q);const I=$.render();I.setAttribute("style","position:absolute;bottom:20px;right:20px"),document.body.append(I)})})()</script><script>((l,U,M,R)=>{const N=l();window.mm=N.Markmap.create("svg#mindmap",(U||N.deriveOptions)(R),M),window.matchMedia("(prefers-color-scheme: dark)").matches&&document.documentElement.classList.add("markmap-dark")})(()=>window.markmap,null,{"content":"&#x1f333; Machine Learning Algorithms Hierarchy","children":[{"content":"1. Machine Learning Algorithms","children":[{"content":"&#x251c;&#x2500;&#x2500; 1.1 Supervised Learning Algorithms","children":[{"content":"&#x251c;&#x2500;&#x2500; 1.1.1 Linear Models","children":[{"content":"Linear Regression: Predict continuous values","children":[],"payload":{"tag":"li","lines":"11,12"}},{"content":"Logistic Regression: Predict binary outcomes","children":[],"payload":{"tag":"li","lines":"12,14"}}],"payload":{"tag":"h4","lines":"10,11"}},{"content":"&#x251c;&#x2500;&#x2500; 1.1.2 Tree-Based Models","children":[{"content":"Decision Tree: Splits data based on feature values","children":[],"payload":{"tag":"li","lines":"15,16"}},{"content":"Random Forest: Ensemble of decision trees","children":[],"payload":{"tag":"li","lines":"16,17"}},{"content":"Gradient Boosting (e.g., XGBoost, LightGBM): Sequential tree learning","children":[],"payload":{"tag":"li","lines":"17,18"}}],"payload":{"tag":"h4","lines":"14,15"}},{"content":"&#x251c;&#x2500;&#x2500; 1.1.3 Support Vector Machines (SVM)","children":[{"content":"Finds optimal hyperplane to separate classes","children":[],"payload":{"tag":"li","lines":"19,20"}},{"content":"Uses kernel trick for non-linear data","children":[],"payload":{"tag":"li","lines":"20,21"}}],"payload":{"tag":"h4","lines":"18,19"}},{"content":"&#x251c;&#x2500;&#x2500; 1.1.4 k-Nearest Neighbors (k-NN)","children":[{"content":"Predicts based on closest data points","children":[],"payload":{"tag":"li","lines":"22,24"}}],"payload":{"tag":"h4","lines":"21,22"}},{"content":"&#x251c;&#x2500;&#x2500; 1.1.5 Naive Bayes","children":[{"content":"Probabilistic model based on Bayes&#x2019; theorem","children":[],"payload":{"tag":"li","lines":"25,26"}},{"content":"Assumes feature independence","children":[],"payload":{"tag":"li","lines":"26,28"}}],"payload":{"tag":"h4","lines":"24,25"}},{"content":"&#x251c;&#x2500;&#x2500; 1.1.6 Neural Networks","children":[{"content":"Composed of layers of interconnected nodes (neurons)","children":[{"content":"Perceptron: Basic unit","children":[],"payload":{"tag":"li","lines":"30,31"}},{"content":"Activation Functions: ReLU, Sigmoid, Tanh","children":[],"payload":{"tag":"li","lines":"31,32"}},{"content":"Loss Functions: Cross-Entropy, MSE","children":[],"payload":{"tag":"li","lines":"32,33"}},{"content":"Optimizers: SGD, Adam","children":[],"payload":{"tag":"li","lines":"33,35"}}],"payload":{"tag":"li","lines":"29,35"}}],"payload":{"tag":"h4","lines":"28,29"}}],"payload":{"tag":"h3","lines":"7,8"}},{"content":"&#x251c;&#x2500;&#x2500; 1.2 Unsupervised Learning Algorithms","children":[{"content":"&#x251c;&#x2500;&#x2500; 1.2.1 Clustering","children":[{"content":"k-Means: Groups data into k clusters","children":[],"payload":{"tag":"li","lines":"41,42"}},{"content":"Hierarchical Clustering: Builds tree of clusters","children":[],"payload":{"tag":"li","lines":"42,43"}},{"content":"DBSCAN: Density-based clustering","children":[],"payload":{"tag":"li","lines":"43,45"}}],"payload":{"tag":"h4","lines":"40,41"}},{"content":"&#x251c;&#x2500;&#x2500; 1.2.2 Dimensionality Reduction","children":[{"content":"PCA (Principal Component Analysis): Projects data to lower dimensions","children":[],"payload":{"tag":"li","lines":"46,47"}},{"content":"t-SNE: Visualizes high-dimensional data","children":[],"payload":{"tag":"li","lines":"47,48"}},{"content":"UMAP: Preserves global structure in embeddings","children":[],"payload":{"tag":"li","lines":"48,50"}}],"payload":{"tag":"h4","lines":"45,46"}},{"content":"&#x251c;&#x2500;&#x2500; 1.2.3 Association Rule Learning","children":[{"content":"Apriori: Finds frequent itemsets","children":[],"payload":{"tag":"li","lines":"51,52"}},{"content":"Eclat: Uses set intersections","children":[],"payload":{"tag":"li","lines":"52,54"}}],"payload":{"tag":"h4","lines":"50,51"}}],"payload":{"tag":"h3","lines":"37,38"}},{"content":"&#x251c;&#x2500;&#x2500; 1.3 Semi-Supervised Learning","children":[{"content":"&#x251c;&#x2500;&#x2500; 1.3.1 Self-training","children":[{"content":"Model trained on labeled data, then predicts labels for unlabeled data","children":[],"payload":{"tag":"li","lines":"60,62"}}],"payload":{"tag":"h4","lines":"59,60"}},{"content":"&#x251c;&#x2500;&#x2500; 1.3.2 Co-training","children":[{"content":"Two models trained on different views of the data","children":[],"payload":{"tag":"li","lines":"63,65"}}],"payload":{"tag":"h4","lines":"62,63"}}],"payload":{"tag":"h3","lines":"56,57"}},{"content":"&#x251c;&#x2500;&#x2500; 1.4 Reinforcement Learning Algorithms","children":[{"content":"&#x251c;&#x2500;&#x2500; 1.4.1 Value-Based Methods","children":[{"content":"Q-Learning: Learn value of actions","children":[],"payload":{"tag":"li","lines":"71,72"}},{"content":"Deep Q-Networks (DQN): Uses neural networks for Q-values","children":[],"payload":{"tag":"li","lines":"72,74"}}],"payload":{"tag":"h4","lines":"70,71"}},{"content":"&#x251c;&#x2500;&#x2500; 1.4.2 Policy-Based Methods","children":[{"content":"REINFORCE: Directly optimizes policy","children":[],"payload":{"tag":"li","lines":"75,76"}},{"content":"Actor-Critic: Combines value and policy learning","children":[],"payload":{"tag":"li","lines":"76,78"}}],"payload":{"tag":"h4","lines":"74,75"}},{"content":"&#x251c;&#x2500;&#x2500; 1.4.3 Model-Based Methods","children":[{"content":"Learn environment dynamics to plan actions","children":[],"payload":{"tag":"li","lines":"79,81"}}],"payload":{"tag":"h4","lines":"78,79"}}],"payload":{"tag":"h3","lines":"67,68"}},{"content":"&#x251c;&#x2500;&#x2500; 1.5 Ensemble Methods","children":[{"content":"&#x251c;&#x2500;&#x2500; 1.5.1 Bagging","children":[{"content":"Random Forest: Reduces variance","children":[],"payload":{"tag":"li","lines":"87,88"}}],"payload":{"tag":"h4","lines":"86,87"}},{"content":"&#x251c;&#x2500;&#x2500; 1.5.2 Boosting","children":[{"content":"AdaBoost, Gradient Boosting: Reduces bias","children":[],"payload":{"tag":"li","lines":"89,91"}}],"payload":{"tag":"h4","lines":"88,89"}},{"content":"&#x251c;&#x2500;&#x2500; 1.5.3 Stacking","children":[{"content":"Combines predictions from multiple models","children":[],"payload":{"tag":"li","lines":"92,94"}}],"payload":{"tag":"h4","lines":"91,92"}}],"payload":{"tag":"h3","lines":"83,84"}},{"content":"&#x251c;&#x2500;&#x2500; 1.6 Deep Learning Architectures","children":[{"content":"&#x251c;&#x2500;&#x2500; 1.6.1 CNN (Convolutional Neural Network)","children":[{"content":"For image data","children":[{"content":"Convolution Layer","children":[],"payload":{"tag":"li","lines":"101,102"}},{"content":"Pooling Layer","children":[],"payload":{"tag":"li","lines":"102,103"}},{"content":"Fully Connected Layer","children":[],"payload":{"tag":"li","lines":"103,105"}}],"payload":{"tag":"li","lines":"100,105"}}],"payload":{"tag":"h4","lines":"99,100"}},{"content":"&#x251c;&#x2500;&#x2500; 1.6.2 RNN (Recurrent Neural Network)","children":[{"content":"For sequential data","children":[{"content":"LSTM: Long-term memory","children":[],"payload":{"tag":"li","lines":"107,108"}},{"content":"GRU: Gated recurrent units","children":[],"payload":{"tag":"li","lines":"108,109"}}],"payload":{"tag":"li","lines":"106,109"}}],"payload":{"tag":"h4","lines":"105,106"}},{"content":"&#x251c;&#x2500;&#x2500; 1.6.3 Transformer","children":[{"content":"For NLP and vision","children":[{"content":"Attention Mechanism","children":[],"payload":{"tag":"li","lines":"111,112"}},{"content":"Positional Encoding","children":[],"payload":{"tag":"li","lines":"112,113"}},{"content":"Encoder-Decoder Architecture","children":[],"payload":{"tag":"li","lines":"113,114"}}],"payload":{"tag":"li","lines":"110,114"}}],"payload":{"tag":"h4","lines":"109,110"}}],"payload":{"tag":"h3","lines":"96,97"}},{"content":"&#x251c;&#x2500;&#x2500; 1.6 Deep Learning Architectures","children":[{"content":"&#x251c;&#x2500;&#x2500; 1.6.1 CNN (Convolutional Neural Network)","children":[{"content":"For image data","children":[{"content":"Convolution Layer","children":[],"payload":{"tag":"li","lines":"119,120"}},{"content":"Pooling Layer","children":[],"payload":{"tag":"li","lines":"120,121"}},{"content":"Fully Connected Layer","children":[],"payload":{"tag":"li","lines":"121,123"}}],"payload":{"tag":"li","lines":"118,123"}}],"payload":{"tag":"h4","lines":"117,118"}},{"content":"&#x251c;&#x2500;&#x2500; 1.6.2 RNN (Recurrent Neural Network)","children":[{"content":"For sequential data","children":[{"content":"LSTM: Long-term memory","children":[],"payload":{"tag":"li","lines":"125,126"}},{"content":"GRU: Gated recurrent units","children":[],"payload":{"tag":"li","lines":"126,128"}}],"payload":{"tag":"li","lines":"124,128"}}],"payload":{"tag":"h4","lines":"123,124"}},{"content":"&#x251c;&#x2500;&#x2500; 1.6.3 Transformer","children":[{"content":"For NLP and vision","children":[{"content":"Attention Mechanism","children":[],"payload":{"tag":"li","lines":"130,131"}},{"content":"Positional Encoding","children":[],"payload":{"tag":"li","lines":"131,132"}},{"content":"Encoder-Decoder Architecture","children":[],"payload":{"tag":"li","lines":"132,134"}}],"payload":{"tag":"li","lines":"129,134"}}],"payload":{"tag":"h4","lines":"128,129"}},{"content":"&#x251c;&#x2500;&#x2500; 1.6.4 GAN (Generative Adversarial Network)","children":[{"content":"Two networks: Generator and Discriminator","children":[],"payload":{"tag":"li","lines":"135,136"}},{"content":"Generator creates fake data, Discriminator evaluates it","children":[],"payload":{"tag":"li","lines":"136,137"}},{"content":"Trained using adversarial loss (minimax game)","children":[],"payload":{"tag":"li","lines":"137,138"}},{"content":"Variants:","children":[{"content":"DCGAN: Deep convolutional GAN","children":[],"payload":{"tag":"li","lines":"139,140"}},{"content":"WGAN: Uses Wasserstein loss for stability","children":[],"payload":{"tag":"li","lines":"140,141"}},{"content":"CycleGAN: Image-to-image translation","children":[],"payload":{"tag":"li","lines":"141,142"}},{"content":"StyleGAN: High-quality image generation","children":[],"payload":{"tag":"li","lines":"142,145"}}],"payload":{"tag":"li","lines":"138,145"}}],"payload":{"tag":"h4","lines":"134,135"}}],"payload":{"tag":"h3","lines":"114,115"}},{"content":"&#x251c;&#x2500;&#x2500; 1.7 Evolutionary Algorithms","children":[{"content":"&#x251c;&#x2500;&#x2500; 1.7.1 Genetic Algorithms","children":[{"content":"Selection, crossover, mutation","children":[],"payload":{"tag":"li","lines":"151,153"}}],"payload":{"tag":"h4","lines":"150,151"}},{"content":"&#x251c;&#x2500;&#x2500; 1.7.2 Particle Swarm Optimization","children":[{"content":"Simulates social behavior of birds/fish","children":[],"payload":{"tag":"li","lines":"154,156"}}],"payload":{"tag":"h4","lines":"153,154"}}],"payload":{"tag":"h3","lines":"147,148"}},{"content":"&#x251c;&#x2500;&#x2500; 1.8 Probabilistic Models","children":[{"content":"&#x251c;&#x2500;&#x2500; 1.8.1 Bayesian Networks","children":[{"content":"Directed acyclic graphs of probabilistic relationships","children":[],"payload":{"tag":"li","lines":"162,163"}}],"payload":{"tag":"h4","lines":"161,162"}},{"content":"&#x251c;&#x2500;&#x2500; 1.8.2 Hidden Markov Models","children":[{"content":"For time-series with hidden states","children":[],"payload":{"tag":"li","lines":"164,166"}}],"payload":{"tag":"h4","lines":"163,164"}}],"payload":{"tag":"h3","lines":"158,159"}}],"payload":{"tag":"h2","lines":"2,3"}}],"payload":{"tag":"h1","lines":"0,1"}},null)</script>
</body>
</html>
